# Dockerfile.triton

# Base image: NVIDIA Triton Inference Server with Python support
FROM nvcr.io/nvidia/tritonserver:23.03-py3

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3-pip \
    python3-venv \
    wget \
    supervisor \
    netcat && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy and install Python dependencies
COPY requirements.txt /app/requirements.txt
RUN python3 -m venv /app/venv && \
    /app/venv/bin/pip install --upgrade pip && \
    /app/venv/bin/pip install -r /app/requirements.txt

# Update PATH to prioritize virtual environment's Python and pip
ENV PATH="/app/venv/bin:$PATH"

# Copy application files and directories
COPY config /app/config
COPY src /app/src
COPY static /app/static
COPY tests /app/tests
COPY pytest.ini /app/pytest.ini
COPY triton_models /models
COPY start_triton.sh /app/start_triton.sh

# Expose necessary ports for FastAPI and Triton Server
EXPOSE 8000 8001 8002 9000

# Define environment variables
ENV STATIC_DIR=/app/static \
    MODEL_PATH=/app/config/pretrained-roberta-base.h5 \
    CONFIG_PATH=/app/config/config-roberta-base.json \
    TOKENIZER_PATH=/app/config/vocab-roberta-base.json \
    MERGES_PATH=/app/config/merges-roberta-base.txt

# Make entrypoint script executable and set entrypoint
RUN chmod +x /app/start_triton.sh
ENTRYPOINT ["/app/start_triton.sh"]
