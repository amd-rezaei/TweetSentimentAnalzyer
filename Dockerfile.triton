# Dockerfile.triton

# Use Triton Inference Server base image with Python
FROM nvcr.io/nvidia/tritonserver:23.03-py3

# Install system dependencies, Miniconda, and any required tools
RUN apt-get update && apt-get install -y wget supervisor netcat && \
    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \
    bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda && \
    rm Miniconda3-latest-Linux-x86_64.sh && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Add Conda to PATH
ENV PATH=/opt/conda/bin:$PATH
ENV RUN_TESTS_ON_START="true"  


# Copy environment.yml to create Conda environment
COPY environment.yml /app/environment.yml
RUN conda env create -f /app/environment.yml && conda clean -afy

# Set working directory
WORKDIR /app

# Copy application files and directories
COPY config /app/config
COPY src /app/src
COPY static /app/static
COPY tests /app/tests
COPY pytest.ini /app/pytest.ini
COPY triton_models /models
COPY start_triton.sh /app/start_triton.sh


# Expose ports for both FastAPI and Triton
EXPOSE 8000 8001 8002 9000

# Set environment variables
ENV STATIC_DIR=/app/static \
    MODEL_PATH=/app/config/pretrained-roberta-base.h5 \
    CONFIG_PATH=/app/config/config-roberta-base.json \
    TOKENIZER_PATH=/app/config/vocab-roberta-base.json \
    MERGES_PATH=/app/config/merges-roberta-base.txt

# Set permissions and entrypoint
RUN chmod +x /app/start_triton.sh
ENTRYPOINT ["/app/start_triton.sh"]

